## Проект.  
Решается задача построения базового пайплайна ETL процесса от получения данных до выгрузки результатов модели в облачное хранилище с помощью Apache Airflow и Python.  
Пайплайн состоит из следующих шагов:  
1. Извлечение данных: выгрузка данных по url https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data
Это известный учебный датасет Breast Cancer Wisconsin (Diagnostic) Data Set, подробное описание которого можно найти, например, тут: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data  ;
3. Преобразование данных: проверка валидности данных, очистка и нормализация данных;  
4. Обучение модели: обучение модели LogisticRegression, получение обченной модели и метрик Accuracy, Precision, Recall, F1;  
5. Выгрузка рельтатов работы модели: загрузка данных на yandexDisk, с использованием api токена. Выгружаются предсказания модели и метрики;  
  
![](https://github.com/AushkpaNS/DE_exam/blob/master/images/pipeline.png)
  
**Структура репозитория:**  
- /results/ - папка с промежуточными артефактами. Состоит из:  
  - /results/extract/ - папка, в которую сохраняется выгруженные данные без обработки в формате .csv;  
  - /results/transform/ - папка, в которую сохраняеются обработанные данные для обучения модели в формате .csv;  
  - /results/model/ - папка, в которую сохраняется обученная модель в формате .pkl;  
  - /results/prediction/ - папка, в которую сохраняется файл с предсказаниями мрдели в формате .csv и файл с названием модели и метриками Accuracy, Precision, Recall, F1 в формате json: {"model": "путь к файлу модели", "Accuracy": 0, "Precision": 0, "Recall": 0, "F1": 0};  
  - /results/archive/ с внутренним набором папок extract, transform, model, prediction - папка, в котору переносятся данные из остальных папок после выгрузки результата в облачное хранилище;  
- /etl/ - папка со скриптами, реализующими процесс:  
  - extract_data.py - скрипт выгружает данные из источника (шаг 1. Извлечение данных) и сохраняет в .csv файл в папку /results/extract/;  
  - transform_data.py - скрипт считывает данные из .csv файла из папки /results/extract/, выполняет форматно логический контроль, очищает данные (удаляет пропуски, удаляет дубли, нормирует данные) и сохраняет подготовленные для обучения модели данные в папку /results/transform/;  
  - train_model.py - скрипт считывает данные из .csv файла из папки /results/transform/, обучает модель LogisticRegression и сохраняет в папку /results/model/ обученную модель;  
  - model_pred.py - скрипт считывает из .pkl файла из папки /results/model/ обученную модель, считывает из .csv файла из папки /results/transform/ данные, получает предсказания и метрики Accuracy, Precision, Recall, F1. Результаты сохраняет в файлы .csv и .json в папку /results/prediction/ ;  
  - load_data.py - скрипт переносит файлы и папки /results/prediction/ в облачное хранилище (yaDisk), с использованием api токена. Затем переносит все файлы в соответствующие папки в /results/archive/ ;
- /logs/ - папка с логом выполнения скриптов;
- /dags/pipeline_dag.py - dag для Apache Airflow (см. схему выше). Скрипты запускаются строго последовательно, запуск раз в день, 3 попытки через 5 минут, таймаут 600.
- config.ini - файл с конфигурационными настройками:  
[BCWD]  
URL = url для выгрузки данных  
LOG_FILE = полный путь к папке /logs/  
EXTRACT_FILE_PATH = полный путь к папке /results/extract/  
TRANSFORM_FILE_PATH = полный путь к папке /results/transform/  
MODEL_FILE_PATH = полный путь к папке /results/model/  
PREDICTION_FILE_PATH = полный путь к папке /results/prediction/  
ARCHIVE_EXTRACT_FILE_PATH = полный путь к папке /results/archive/extract/  
ARCHIVE_TRANSFORM_FILE_PATH = полный путь к папке /results/archive/transform/  
ARCHIVE_MODEL_FILE_PATH = полный путь к папке /results/archive/model/  
ARCHIVE_PREDICTION_FILE_PATH = полный путь к папке /results/archive/prediction/  
YA_DISK_API_TOKEN = api токен для подключения к yaDisk  
YA_DISK_DEST_PATH = директория назначенив в yaDisk  

**Инфраструктура проекта**  
Для запуска проекта было использовано облачное решение IaaS: YandexCloud.  
В YandexCloud была создана виртуальная машина.  
На виртуальную машину был установлен python версии 3.8, mysqlclient версии 2.2.4, apache airflow версии 2.7.3.
Управление dag осуществляется через веб-интерфейс apache airflow.
![](https://github.com/AushkpaNS/DE_exam/blob/master/images/im1.png)
![](https://github.com/AushkpaNS/DE_exam/blob/master/images/im2.png)

**Ограничения релазиции**  
Релазован только базовый пайплайн. Поэтому:  
- реализованы тривиальные методы форматно логического контроля и подготовки данных для модели.  
- реализовано тривиальное обучение модели LogisticRegression без разделения на тренировочную и тестовую выборки, без подбора гиперпараметров и т.д.  
- испольузется промежуточное файловое хранилище, а не база данных.  
- не реализован поток, который можно было бы настроить параллельно: получение предсказаний при помощи актуальной модели для не размеченных данных.  
- не реализовван контроль качества обчения модели (лучше ли предыдущего раза или нет и т.п.)

**Анализ ошибок и устойчивости**  
- Где может «упасть» процесс?  
Процесс может "упасть", если не доступны данные по url или не доступно целевое облачное хранилище для выгрузки результатов.  
Так же процесс может "упасть" при изменении формата данных в источнике.  
Так же процесс может "упасть" в случае ошибочного параллельного запуска дубля процесса.  
- Какие исключения могут возникнуть?  
См. ответ на первый вопрос.  
- Что произойдет при потере соединения с источником данных?  
Процесс в airflow попробует повторить первый шаг (Извлечение данных: запуск скрипта extract_data.py) в соответствии с настройками (3 раза через 5 минут) и, в случае неудачи, остановится.  
- Что будет, если источник отдает невалидные данные?  
В скипте extract_data.py возникнет исключение, которое airflow воспримет как ошибку.  
- Что произойдет, если модель не обучается или выдает ошибку?  
В скипте train_model.py возникнет исключение, которое airflow воспримет как ошибку.  


